{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shalabh147/Brain-Tumor-Segmentation-and-Survival-Prediction-using-Deep-Neural-Networks/blob/master/surv_pred_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "ghhRoTgL1I2J"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "# import keras\n",
    "# from ensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout,Maximum,Flatten\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose,Conv3D,Conv3DTranspose,UpSampling2D\n",
    "# from tensorflow.keras.layers.convolutional import Conv2D, Conv2DTranspose,Conv3D,Conv3DTranspose,UpSampling2D\n",
    "# from tensorflow.keras.layers.convolutional import Conv2D, Conv2DTranspose,Conv3D,Conv3DTranspose,UpSampling2D\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D,MaxPooling3D,AveragePooling2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from sklearn.utils import class_weight\n",
    "from keras.models import Sequential\n",
    "import nibabel as nib\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "path1 = '/home/vivek/Desktop/BTP_2020/Segmentation/dataset/MICCAI_BraTS_2018_Data_Training/survival_data.csv'\n",
    "#print(os.listdir(path))\n",
    "# Brats18_CBICA_ANP_1\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "lAyichNT1I2a",
    "outputId": "ea77406a-13b0-4b0b-e7f3-bf7bbf7edd43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are BraTS18ID, Age, Survival, ResectionStatus\n",
      "['Brats18_TCIA08_167_1', '74.907', '153', 'NA']\n",
      "['Brats18_TCIA08_242_1', '66.479', '147', 'NA']\n",
      "['Brats18_TCIA08_319_1', '64.86', '254', 'NA']\n",
      "['Brats18_TCIA08_469_1', '63.899', '519', 'NA']\n",
      "['Brats18_TCIA08_218_1', '57.345', '346', 'NA']\n",
      "['Brats18_TCIA08_406_1', '78.745', '82', 'NA']\n",
      "['Brats18_TCIA08_280_1', '57.362', '508', 'NA']\n",
      "['Brats18_TCIA08_105_1', '66.627', '77', 'NA']\n",
      "['Brats18_TCIA08_278_1', '50.501', '1458', 'NA']\n",
      "['Brats18_TCIA06_247_1', '76.699', '244', 'NA']\n",
      "['Brats18_TCIA06_372_1', '74.521', '213', 'NA']\n",
      "['Brats18_TCIA06_165_1', '51.756', '5', 'NA']\n",
      "['Brats18_TCIA06_409_1', '69.266', '99', 'NA']\n",
      "['Brats18_TCIA06_184_1', '61.167', '434', 'NA']\n",
      "['Brats18_TCIA05_277_1', '70.367', '232', 'NA']\n",
      "['Brats18_TCIA05_478_1', '59.255', '30', 'NA']\n",
      "['Brats18_TCIA04_437_1', '46.953', '333', 'NA']\n",
      "['Brats18_TCIA04_361_1', '75.973', '476', 'NA']\n",
      "['Brats18_TCIA04_192_1', '75.962', '121', 'NA']\n",
      "['Brats18_TCIA04_479_1', '56.4', '372', 'NA']\n",
      "['Brats18_TCIA04_111_1', '75.263', '626', 'NA']\n",
      "['Brats18_TCIA04_343_1', '52.679', '296', 'NA']\n",
      "['Brats18_TCIA04_149_1', '36.852', '448', 'NA']\n",
      "['Brats18_TCIA03_474_1', '61.408', '635', 'NA']\n",
      "['Brats18_TCIA03_419_1', '70.592', '327', 'NA']\n",
      "['Brats18_TCIA03_199_1', '48.825', '1282', 'NA']\n",
      "['Brats18_TCIA03_133_1', '63.762', '382', 'NA']\n",
      "['Brats18_TCIA03_296_1', '60.427', '22', 'NA']\n",
      "['Brats18_TCIA03_257_1', '69.326', '425', 'NA']\n",
      "['Brats18_TCIA03_498_1', '59.282', '467', 'NA']\n",
      "['Brats18_TCIA03_138_1', '71.874', '82', 'NA']\n",
      "['Brats18_TCIA03_338_1', '76.425', '468', 'NA']\n",
      "['Brats18_TCIA03_265_1', '59.584', '103', 'NA']\n",
      "['Brats18_TCIA03_375_1', '60', '946', 'NA']\n",
      "['Brats18_TCIA03_121_1', '30.408', '747', 'NA']\n",
      "['Brats18_TCIA02_274_1', '54.967', '357', 'NA']\n",
      "['Brats18_TCIA02_473_1', '61.022', '175', 'NA']\n",
      "['Brats18_TCIA02_322_1', '57.362', '621', 'NA']\n",
      "['Brats18_TCIA02_179_1', '46.677', '405', 'NA']\n",
      "['Brats18_TCIA02_368_1', '62.562', '317', 'NA']\n",
      "['Brats18_TCIA02_135_1', '69.364', '828', 'NA']\n",
      "['Brats18_TCIA02_471_1', '76.614', '111', 'NA']\n",
      "['Brats18_TCIA02_394_1', '64.247', '616', 'NA']\n",
      "['Brats18_TCIA02_300_1', '64.378', '127', 'NA']\n",
      "['Brats18_TCIA02_151_1', '47.973', '1731', 'NA']\n",
      "['Brats18_TCIA02_118_1', '47.321', '104', 'NA']\n",
      "['Brats18_TCIA02_226_1', '73.578', '329', 'NA']\n",
      "['Brats18_TCIA02_455_1', '54.844', '424', 'NA']\n",
      "['Brats18_TCIA02_283_1', '74.836', '262', 'NA']\n",
      "['Brats18_TCIA02_430_1', '53.866', '71', 'NA']\n",
      "['Brats18_TCIA02_321_1', '81.211', '67', 'NA']\n",
      "['Brats18_TCIA02_314_1', '40.353', '362', 'NA']\n",
      "['Brats18_TCIA02_290_1', '43.112', '737', 'NA']\n",
      "['Brats18_TCIA02_377_1', '63.762', '812', 'NA']\n",
      "['Brats18_TCIA02_198_1', '54.279', '394', 'NA']\n",
      "['Brats18_TCIA02_331_1', '84.844', '187', 'NA']\n",
      "['Brats18_TCIA02_491_1', '81.112', '82', 'NA']\n",
      "['Brats18_TCIA01_150_1', '51.115', '1489', 'NA']\n",
      "['Brats18_TCIA01_335_1', '54.474', '355', 'NA']\n",
      "['Brats18_TCIA01_411_1', '42.904', '822', 'NA']\n",
      "['Brats18_TCIA01_203_1', '45.926', '268', 'NA']\n",
      "['Brats18_TCIA01_231_1', '63.805', '1561', 'NA']\n",
      "['Brats18_TCIA01_390_1', '63.575', '634', 'NA']\n",
      "['Brats18_TCIA01_235_1', '57.973', '804', 'NA']\n",
      "['Brats18_TCIA01_499_1', '50.082', '600', 'NA']\n",
      "['Brats18_TCIA01_412_1', '68.759', '291', 'NA']\n",
      "['Brats18_TCIA01_448_1', '44.449', '199', 'NA']\n",
      "['Brats18_TCIA01_401_1', '78.792', '448', 'NA']\n",
      "['Brats18_TCIA01_147_1', '61.416', '209', 'NA']\n",
      "['Brats18_TCIA01_378_1', '74.145', '110', 'NA']\n",
      "['Brats18_TCIA01_201_1', '60.729', '430', 'NA']\n",
      "['Brats18_TCIA01_429_1', '54.986', '86', 'NA']\n",
      "['Brats18_TCIA01_186_1', '33.888', '370', 'NA']\n",
      "['Brats18_TCIA01_460_1', '18.975', '630', 'NA']\n",
      "['Brats18_TCIA01_190_1', '61.526', '322', 'NA']\n",
      "['Brats18_TCIA01_425_1', '56.208', '558', 'NA']\n",
      "['Brats18_2013_11_1', '29.12', '150', 'GTR']\n",
      "['Brats18_2013_27_1', '68.02', '120', 'GTR']\n",
      "['Brats18_CBICA_BHM_1', '62.03', '436', 'STR']\n",
      "['Brats18_CBICA_BHB_1', '55.595', '510', 'GTR']\n",
      "['Brats18_CBICA_AZH_1', '54.915', '401', 'GTR']\n",
      "['Brats18_CBICA_AZD_1', '46.258', '448', 'GTR']\n",
      "['Brats18_CBICA_AYW_1', '49.874', '734', 'NA']\n",
      "['Brats18_CBICA_AYU_1', '63.781', '58', 'GTR']\n",
      "['Brats18_CBICA_AYI_1', '65.921', '387', 'GTR']\n",
      "['Brats18_CBICA_AYA_1', '74.836', '50', 'GTR']\n",
      "['Brats18_CBICA_AXW_1', '79.211', '191', 'GTR']\n",
      "['Brats18_CBICA_AXQ_1', '66.282', '114', 'GTR']\n",
      "['Brats18_CBICA_AXO_1', '56.301', '394', 'NA']\n",
      "['Brats18_CBICA_AXN_1', '85.762', '345', 'GTR']\n",
      "['Brats18_CBICA_AXM_1', '66.934', '438', 'STR']\n",
      "['Brats18_CBICA_AXL_1', '74.63', '168', 'GTR']\n",
      "['Brats18_CBICA_AXJ_1', '27.811', '1767', 'GTR']\n",
      "['Brats18_CBICA_AWI_1', '46.551', '375', 'GTR']\n",
      "['Brats18_CBICA_AWH_1', '52.764', '139', 'GTR']\n",
      "['Brats18_CBICA_AWG_1', '55.532', '180', 'GTR']\n",
      "['Brats18_CBICA_AVV_1', '72.293', '387', 'GTR']\n",
      "['Brats18_CBICA_AVJ_1', '45.244', '614', 'GTR']\n",
      "['Brats18_CBICA_AVG_1', '63.359', '579', 'GTR']\n",
      "['Brats18_CBICA_AUR_1', '70.252', '12', 'GTR']\n",
      "['Brats18_CBICA_AUQ_1', '60.816', '1337', 'NA']\n",
      "['Brats18_CBICA_AUN_1', '68.504', '376', 'GTR']\n",
      "['Brats18_CBICA_ATX_1', '36.784', '1592', 'GTR']\n",
      "['Brats18_CBICA_ATV_1', '62.159', '453', 'GTR']\n",
      "['Brats18_CBICA_ATP_1', '51.589', '385', 'GTR']\n",
      "['Brats18_CBICA_ATF_1', '68.726', '152', 'GTR']\n",
      "['Brats18_CBICA_ATD_1', '69.178', '355', 'GTR']\n",
      "['Brats18_CBICA_ATB_1', '71.126', '208', 'GTR']\n",
      "['Brats18_CBICA_ASY_1', '66.51', '610', 'STR']\n",
      "['Brats18_CBICA_ASW_1', '68.359', '239', 'GTR']\n",
      "['Brats18_CBICA_ASV_1', '54.751', '597', 'GTR']\n",
      "['Brats18_CBICA_ASU_1', '81.285', '85', 'STR']\n",
      "['Brats18_CBICA_ASO_1', '52.348', '265', 'STR']\n",
      "['Brats18_CBICA_ASN_1', '39.488', '407', 'STR']\n",
      "['Brats18_CBICA_ASK_1', '77.337', '522', 'GTR']\n",
      "['Brats18_CBICA_ASH_1', '46.57', '660', 'GTR']\n",
      "['Brats18_CBICA_ASG_1', '57.71', '208', 'STR']\n",
      "['Brats18_CBICA_ASE_1', '46.814', '318', 'STR']\n",
      "['Brats18_CBICA_ASA_1', '63.764', '210', 'STR']\n",
      "['Brats18_CBICA_ARZ_1', '54.825', '871', 'STR']\n",
      "['Brats18_CBICA_ARW_1', '44.416', '495', 'STR']\n",
      "['Brats18_CBICA_ARF_1', '75.312', '726', 'GTR']\n",
      "['Brats18_CBICA_AQZ_1', '63.345', '286', 'STR']\n",
      "['Brats18_CBICA_AQY_1', '57.718', '229', 'STR']\n",
      "['Brats18_CBICA_AQV_1', '53.362', '84', 'GTR']\n",
      "['Brats18_CBICA_AQU_1', '72.879', '30', 'STR']\n",
      "['Brats18_CBICA_AQT_1', '75.978', '172', 'GTR']\n",
      "['Brats18_CBICA_AQR_1', '71.37', '89', 'GTR']\n",
      "['Brats18_CBICA_AQQ_1', '69.992', '33', 'STR']\n",
      "['Brats18_CBICA_AQP_1', '46.452', '1283', 'GTR']\n",
      "['Brats18_CBICA_AQO_1', '67.86', '473', 'GTR']\n",
      "['Brats18_CBICA_AQN_1', '63.192', '488', 'STR']\n",
      "['Brats18_CBICA_AQJ_1', '66.074', '170', 'STR']\n",
      "['Brats18_CBICA_AQG_1', '53.605', '466', 'STR']\n",
      "['Brats18_CBICA_AQD_1', '73.036', '32', 'STR']\n",
      "['Brats18_CBICA_AQA_1', '76.367', '106', 'GTR']\n",
      "['Brats18_CBICA_APZ_1', '60.063', '336', 'STR']\n",
      "['Brats18_CBICA_APY_1', '45.548', '203', 'STR']\n",
      "['Brats18_CBICA_APR_1', '62.704', '23', 'STR']\n",
      "['Brats18_CBICA_AOZ_1', '46.666', '331', 'GTR']\n",
      "['Brats18_CBICA_AOP_1', '67.833', '332', 'GTR']\n",
      "['Brats18_CBICA_AOO_1', '44.162', '350', 'GTR']\n",
      "['Brats18_CBICA_AOH_1', '56.921', '576', 'GTR']\n",
      "['Brats18_CBICA_AOD_1', '60.581', '55', 'NA']\n",
      "['Brats18_CBICA_ANZ_1', '68.049', '287', 'GTR']\n",
      "['Brats18_CBICA_ANP_1', '61.605', '486', 'GTR']\n",
      "['Brats18_CBICA_ANI_1', '58.258', '439', 'GTR']\n",
      "['Brats18_CBICA_ANG_1', '55.759', '368', 'GTR']\n",
      "['Brats18_CBICA_AMH_1', '62.614', '169', 'GTR']\n",
      "['Brats18_CBICA_AME_1', '51.734', '359', 'GTR']\n",
      "['Brats18_CBICA_ALX_1', '59.693', '698', 'GTR']\n",
      "['Brats18_CBICA_ALU_1', '65.899', '495', 'GTR']\n",
      "['Brats18_CBICA_ALN_1', '60.942', '421', 'STR']\n",
      "['Brats18_CBICA_ABY_1', '48.367', '515', 'GTR']\n",
      "['Brats18_CBICA_ABO_1', '56.419', '1155', 'GTR']\n",
      "['Brats18_CBICA_ABN_1', '68.285', '1278', 'STR']\n",
      "['Brats18_CBICA_ABM_1', '69.912', '503', 'GTR']\n",
      "['Brats18_CBICA_ABE_1', '67.126', '269', 'GTR']\n",
      "['Brats18_CBICA_ABB_1', '68.493', '465', 'GTR']\n",
      "['Brats18_CBICA_AAP_1', '39.068', '788', 'GTR']\n",
      "['Brats18_CBICA_AAL_1', '54.301', '464', 'GTR']\n",
      "['Brats18_CBICA_AAG_1', '52.263', '616', 'GTR']\n",
      "['Brats18_CBICA_AAB_1', '60.463', '289', 'GTR']\n",
      "Processed 164 lines.\n",
      "55 64 44\n",
      "1767\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "#import pickle\n",
    "\n",
    "#from joblib import dump\n",
    "\n",
    "age_dict = {}\n",
    "days_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(path1, mode='r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file,delimiter = ',')\n",
    "    line_count = 0\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    max_days = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            print(row)\n",
    "            key = row[0]\n",
    "            age = row[1]\n",
    "            days = row[2]\n",
    "            age_dict[key] = float(age)\n",
    "            days_dict[key] = int(days)\n",
    "            max_days = max(max_days,int(days))\n",
    "            if int(days) < 250:\n",
    "                a += 1\n",
    "            elif (int(days) >= 250 and int(days) <= 500):\n",
    "                b += 1\n",
    "            else:\n",
    "                c += 1\n",
    "            line_count+=1\n",
    "\n",
    "    print(f'Processed {line_count} lines.')\n",
    "    #age_m = np.zeros((1,1))\n",
    "    print(a,b,c)\n",
    "    print(max_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DfXkPloH1I2p"
   },
   "outputs": [],
   "source": [
    "def SurvPredNet(input_img,age_m):\n",
    "    #input_img = BatchNormalization()(input_img)\n",
    "    a1 = Conv2D(16,kernel_size = (3,3) , padding='same')(input_img)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    \n",
    "    a1 = MaxPooling2D(pool_size = (2,2) , strides = (2,2))(a1)\n",
    "    \n",
    "    a1 = Conv2D(24,kernel_size = (3,3) , padding='same')(a1)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    \n",
    "    a1 = MaxPooling2D(pool_size = (2,2) , strides = (2,2))(a1)\n",
    "    \n",
    "    a1 = Conv2D(32,kernel_size = (3,3) , padding='same')(a1)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    \n",
    "    a1 = MaxPooling2D(pool_size = (2,2) , strides = (2,2))(a1)\n",
    "    \n",
    "    a1 = Conv2D(32,kernel_size = (3,3) , padding='same')(a1)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    \n",
    "    a1 = MaxPooling2D(pool_size = (2,2) , strides = (2,2))(a1)\n",
    "    \n",
    "    a1 = Conv2D(32,kernel_size = (3,3) , padding='same')(a1)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    \n",
    "    a1 = AveragePooling2D(pool_size = (6,6) , strides = (1,1))(a1)\n",
    "    \n",
    "    a1 = Conv2D(32,kernel_size = (3,3) , padding = 'same')(a1)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    \n",
    "    a1 = AveragePooling2D(pool_size = (5,5) , strides = (1,1))(a1)\n",
    "    a1 = Conv2D(32,kernel_size = (3,3) , padding = 'same')(a1)\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    a1 = Activation('relu')(a1)\n",
    "    \n",
    "    a1 = AveragePooling2D(pool_size = (6,6) , strides = (1,1))(a1)\n",
    "    \n",
    "    a1 = Flatten()(a1)\n",
    "    a1 = concatenate([a1,age_m])\n",
    "    a1 = BatchNormalization()(a1)\n",
    "    \n",
    "    a1 = Dense(32,activation = 'relu')(a1)\n",
    "    a1 = Dense(16,activation = 'relu')(a1)\n",
    "    outputs = Dense(1,activation = 'sigmoid')(a1)\n",
    "    \n",
    "    model = Model(inputs = [input_img,age_m] , outputs = outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mN5yEJpP1I2x",
    "outputId": "6b4d3378-fcd5-46df-e52f-c7ebe85c049b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 240, 240, 5) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 240, 240, 16) 736         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 240, 240, 16) 64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 240, 240, 16) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 120, 120, 16) 0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 120, 120, 24) 3480        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 120, 120, 24) 96          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 120, 120, 24) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 60, 60, 24)   0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 60, 60, 32)   6944        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 60, 60, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 60, 60, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 30, 30, 32)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 30, 30, 32)   9248        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 30, 30, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 30, 30, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 15, 15, 32)   0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 15, 15, 32)   9248        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 15, 15, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 15, 15, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 10, 10, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 10, 10, 32)   9248        average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 10, 10, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 10, 10, 32)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 6, 6, 32)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 6, 6, 32)     9248        average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 6, 6, 32)     128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 6, 6, 32)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 1, 1, 32)     0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32)           0           average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 33)           0           flatten_1[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 33)           132         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           1088        batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           528         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            17          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 50,717\n",
      "Trainable params: 50,251\n",
      "Non-trainable params: 466\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input((240,240,5))\n",
    "age_m = Input((1))\n",
    "model = SurvPredNet(input_img,age_m)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-vYbDAG1I23"
   },
   "outputs": [],
   "source": [
    "path = '/home/vivek/Desktop/BTP_2020/Segmentation/dataset/MICCAI_BraTS_2018_Data_Training/HGG'\n",
    "all_images = os.listdir(path)\n",
    "#print(len(all_images))\n",
    "all_images.sort()\n",
    "\n",
    "def standardize(image):\n",
    "\n",
    "  standardized_image = np.zeros(image.shape)\n",
    "\n",
    "  #\n",
    " \n",
    "      # iterate over the `z` dimension\n",
    "  for z in range(image.shape[2]):\n",
    "      # get a slice of the image\n",
    "      # at channel c and z-th dimension `z`\n",
    "      image_slice = image[:,:,z]\n",
    "\n",
    "      # subtract the mean from image_slice\n",
    "      centered = image_slice - np.mean(image_slice)\n",
    "     \n",
    "      # divide by the standard deviation (only if it is different from zero)\n",
    "      if(np.std(centered)!=0):\n",
    "          centered = centered/np.std(centered)\n",
    "\n",
    "      # update  the slice of standardized image\n",
    "      # with the scaled centered and scaled image\n",
    "      standardized_image[:, :, z] = centered\n",
    "\n",
    "  ### END CODE HERE ###\n",
    "\n",
    "  return standardized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "Brats18_2013_10_1\n",
      "Brats18_2013_11_1\n",
      "Brats18_2013_12_1\n",
      "Brats18_2013_13_1\n",
      "Brats18_2013_14_1\n",
      "Brats18_2013_17_1\n",
      "Brats18_2013_18_1\n",
      "Brats18_2013_19_1\n",
      "Brats18_2013_20_1\n",
      "Brats18_2013_21_1\n",
      "Brats18_2013_22_1\n",
      "Brats18_2013_23_1\n",
      "Brats18_2013_25_1\n",
      "Brats18_2013_26_1\n",
      "Brats18_2013_27_1\n",
      "Brats18_2013_3_1\n",
      "Brats18_2013_4_1\n",
      "Brats18_2013_5_1\n",
      "Brats18_2013_7_1\n",
      "Brats18_CBICA_AAB_1\n",
      "Brats18_CBICA_AAG_1\n",
      "Brats18_CBICA_AAL_1\n",
      "Brats18_CBICA_AAP_1\n",
      "Brats18_CBICA_ABB_1\n",
      "Brats18_CBICA_ABE_1\n",
      "Brats18_CBICA_ABM_1\n",
      "Brats18_CBICA_ABN_1\n",
      "Brats18_CBICA_ABO_1\n",
      "Brats18_CBICA_ABY_1\n",
      "Brats18_CBICA_ALN_1\n",
      "Brats18_CBICA_ALU_1\n",
      "Brats18_CBICA_ALX_1\n",
      "Brats18_CBICA_AME_1\n",
      "Brats18_CBICA_AMH_1\n",
      "Brats18_CBICA_ANG_1\n",
      "Brats18_CBICA_ANI_1\n",
      "Brats18_CBICA_ANP_1\n",
      "Brats18_CBICA_ANZ_1\n",
      "Brats18_CBICA_AOD_1\n",
      "Brats18_CBICA_AOH_1\n",
      "Brats18_CBICA_AOO_1\n",
      "Brats18_CBICA_AOP_1\n",
      "Brats18_CBICA_AOZ_1\n",
      "Brats18_CBICA_APR_1\n",
      "Brats18_CBICA_APY_1\n",
      "Brats18_CBICA_APZ_1\n",
      "Brats18_CBICA_AQA_1\n",
      "Brats18_CBICA_AQD_1\n",
      "Brats18_CBICA_AQG_1\n",
      "Brats18_CBICA_AQJ_1\n",
      "Brats18_CBICA_AQN_1\n",
      "Brats18_CBICA_AQO_1\n",
      "Brats18_CBICA_AQP_1\n",
      "Brats18_CBICA_AQQ_1\n",
      "Brats18_CBICA_AQR_1\n",
      "Brats18_CBICA_AQT_1\n",
      "Brats18_CBICA_AQU_1\n",
      "Brats18_CBICA_AQV_1\n",
      "Brats18_CBICA_AQY_1\n",
      "Brats18_CBICA_AQZ_1\n",
      "Brats18_CBICA_ARF_1\n",
      "Brats18_CBICA_ARW_1\n",
      "Brats18_CBICA_ARZ_1\n",
      "Brats18_CBICA_ASA_1\n",
      "Brats18_CBICA_ASE_1\n",
      "Brats18_CBICA_ASG_1\n",
      "Brats18_CBICA_ASH_1\n",
      "Brats18_CBICA_ASK_1\n",
      "Brats18_CBICA_ASN_1\n",
      "Brats18_CBICA_ASO_1\n",
      "Brats18_CBICA_ASU_1\n",
      "Brats18_CBICA_ASV_1\n",
      "Brats18_CBICA_ASW_1\n",
      "Brats18_CBICA_ASY_1\n",
      "Brats18_CBICA_ATB_1\n",
      "Brats18_CBICA_ATD_1\n",
      "Brats18_CBICA_ATF_1\n",
      "Brats18_CBICA_ATP_1\n",
      "Brats18_CBICA_ATV_1\n",
      "Brats18_CBICA_ATX_1\n",
      "Brats18_CBICA_AUN_1\n",
      "Brats18_CBICA_AUQ_1\n",
      "Brats18_CBICA_AUR_1\n",
      "Brats18_CBICA_AVG_1\n",
      "Brats18_CBICA_AVJ_1\n",
      "Brats18_CBICA_AVV_1\n",
      "Brats18_CBICA_AWG_1\n",
      "Brats18_CBICA_AWH_1\n",
      "Brats18_CBICA_AWI_1\n",
      "Brats18_CBICA_AXJ_1\n",
      "Brats18_CBICA_AXL_1\n",
      "Brats18_CBICA_AXM_1\n",
      "Brats18_CBICA_AXN_1\n",
      "Brats18_CBICA_AXO_1\n",
      "Brats18_CBICA_AXQ_1\n",
      "Brats18_CBICA_AXW_1\n",
      "Brats18_CBICA_AYA_1\n",
      "Brats18_CBICA_AYI_1\n",
      "Brats18_CBICA_AYU_1\n",
      "Brats18_CBICA_AYW_1\n",
      "Brats18_CBICA_AZD_1\n",
      "Brats18_CBICA_AZH_1\n",
      "Brats18_CBICA_BFB_1\n",
      "Brats18_CBICA_BFP_1\n",
      "Brats18_CBICA_BHB_1\n",
      "Brats18_CBICA_BHK_1\n",
      "Brats18_CBICA_BHM_1\n",
      "Brats18_TCIA01_131_1\n",
      "Brats18_TCIA01_147_1\n",
      "Brats18_TCIA01_150_1\n",
      "Brats18_TCIA01_180_1\n",
      "Brats18_TCIA01_186_1\n",
      "Brats18_TCIA01_190_1\n",
      "Brats18_TCIA01_201_1\n",
      "Brats18_TCIA01_203_1\n",
      "Brats18_TCIA01_221_1\n",
      "Brats18_TCIA01_231_1\n",
      "Brats18_TCIA01_235_1\n",
      "Brats18_TCIA01_335_1\n",
      "Brats18_TCIA01_378_1\n",
      "Brats18_TCIA01_390_1\n",
      "Brats18_TCIA01_401_1\n",
      "Brats18_TCIA01_411_1\n",
      "Brats18_TCIA01_412_1\n",
      "Brats18_TCIA01_425_1\n",
      "Brats18_TCIA01_429_1\n",
      "Brats18_TCIA01_448_1\n",
      "Brats18_TCIA01_460_1\n",
      "Brats18_TCIA01_499_1\n",
      "Brats18_TCIA02_117_1\n",
      "Brats18_TCIA02_118_1\n",
      "Brats18_TCIA02_135_1\n",
      "Brats18_TCIA02_151_1\n",
      "Brats18_TCIA02_168_1\n",
      "Brats18_TCIA02_171_1\n",
      "Brats18_TCIA02_179_1\n",
      "Brats18_TCIA02_198_1\n",
      "Brats18_TCIA02_208_1\n",
      "Brats18_TCIA02_222_1\n",
      "Brats18_TCIA02_226_1\n",
      "Brats18_TCIA02_274_1\n",
      "Brats18_TCIA02_283_1\n",
      "Brats18_TCIA02_290_1\n",
      "Brats18_TCIA02_300_1\n",
      "Brats18_TCIA02_309_1\n",
      "Brats18_TCIA02_314_1\n",
      "Brats18_TCIA02_321_1\n",
      "Brats18_TCIA02_322_1\n",
      "Brats18_TCIA02_331_1\n",
      "Brats18_TCIA02_368_1\n",
      "Brats18_TCIA02_370_1\n",
      "Brats18_TCIA02_374_1\n",
      "Brats18_TCIA02_377_1\n",
      "Brats18_TCIA02_394_1\n",
      "Brats18_TCIA02_430_1\n",
      "Brats18_TCIA02_455_1\n",
      "Brats18_TCIA02_471_1\n",
      "Brats18_TCIA02_473_1\n",
      "Brats18_TCIA02_491_1\n",
      "Brats18_TCIA02_605_1\n",
      "Brats18_TCIA02_606_1\n",
      "Brats18_TCIA02_607_1\n",
      "Brats18_TCIA02_608_1\n",
      "Brats18_TCIA03_121_1\n",
      "Brats18_TCIA03_133_1\n",
      "Brats18_TCIA03_138_1\n",
      "Brats18_TCIA03_199_1\n",
      "Brats18_TCIA03_257_1\n",
      "Brats18_TCIA03_265_1\n",
      "Brats18_TCIA03_296_1\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "print(len(all_images))\n",
    "for image_num in range(170):\n",
    "    image1 = all_images[image_num]\n",
    "    print(image1)\n",
    "    if image1 in days_dict:\n",
    "        cnt += 1\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AqWZ3Mf1I28",
    "outputId": "5abf322a-f48c-4c9c-b20f-47d4e1672085"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-a7dad93bb52e>:42: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  image_data = img.get_data()\n",
      "<ipython-input-25-a7dad93bb52e>:37: DeprecationWarning: get_data() is deprecated in favor of get_fdata(), which has a more predictable return type. To obtain get_data() behavior going forward, use numpy.asanyarray(img.dataobj).\n",
      "\n",
      "* deprecated from version: 3.0\n",
      "* Will raise <class 'nibabel.deprecator.ExpiredDeprecationError'> as of version: 5.0\n",
      "  image_data2 = img.get_data()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 134 is out of bounds for axis 0 with size 134",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a7dad93bb52e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mimage_data2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_data2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0minput_to_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0minput_to_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_data2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#age = np.zeros((1,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 134 is out of bounds for axis 0 with size 134"
     ]
    }
   ],
   "source": [
    "loss_hist = []\n",
    "accu_hist = []\n",
    "epoch_wise_loss = []\n",
    "epoch_wise_accu = []\n",
    "#for epochs in range(45):\n",
    "epoch_loss = 0\n",
    "epoch_accu = 0\n",
    "input_to_model = np.zeros((135,240,240,5))\n",
    "age = np.zeros((135,1))\n",
    "ground_truth = np.zeros((135,1))\n",
    "cnt = 0\n",
    "for image_num in range(170):\n",
    "    #print(epochs)\n",
    "    #print(\"image_num \",image_num)\n",
    "    #print(\"cnt\" ,cnt)\n",
    "    data = np.zeros((240,240,155,4))\n",
    "    image_data2=np.zeros((240,240,155))\n",
    "\n",
    "    # data preprocessing starts here\n",
    "\n",
    "    x = all_images[image_num]\n",
    "    \n",
    "    if x in days_dict:\n",
    "        #print(cnt)\n",
    "        #print(x)\n",
    "        folder_path = path + '/' + x;\n",
    "        modalities = os.listdir(folder_path)\n",
    "        modalities.sort()\n",
    "        #data = []\n",
    "        w = 0\n",
    "        for j in range(len(modalities)):\n",
    "          #print(modalities[j])\n",
    "\n",
    "          image_path = folder_path + '/' + modalities[j]\n",
    "          if not(image_path.find('seg.nii') == -1):\n",
    "            img = nib.load(image_path);\n",
    "            image_data2 = img.get_data()\n",
    "            image_data2 = np.asarray(image_data2)\n",
    "            #print(\"Entered ground truth\")\n",
    "          else:\n",
    "            img = nib.load(image_path);\n",
    "            image_data = img.get_data()\n",
    "            image_data = np.asarray(image_data)\n",
    "            image_data = standardize(image_data)\n",
    "            data[:,:,:,w] = image_data\n",
    "            #print(\"Entered modality\")\n",
    "            w = w+1\n",
    "\n",
    "        #print(data.shape)\n",
    "        #print(image_data2.shape)\n",
    "        image_data2[image_data2 == 4] = 3\n",
    "\n",
    "        input_to_model[cnt,:,:,:4] = data[:,:,75,:]\n",
    "        input_to_model[cnt,:,:,4] = image_data2[:,:,75]\n",
    "        #age = np.zeros((1,1))\n",
    "        age[cnt,0] = float(age_dict[x])\n",
    "        days = int(days_dict[x])\n",
    "        \n",
    "\n",
    "        #ground_truth = np.zeros((1,3))\n",
    "        #print(age[cnt,0])\n",
    "        '''\n",
    "        if int(days) < 300:\n",
    "            ground_truth[cnt,0] = 1\n",
    "        elif (int(days) >= 300 and int(days) <= 450):\n",
    "            ground_truth[cnt,1] = 1\n",
    "        else:\n",
    "            ground_truth[cnt,2] = 1 \n",
    "        '''\n",
    "        ground_truth[cnt,0] = int(days)/max_days\n",
    "        #print(ground_truth[cnt])\n",
    "        cnt += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AqWZ3Mf1I28",
    "outputId": "5abf322a-f48c-4c9c-b20f-47d4e1672085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "3/3 [==============================] - 6s 1s/step - loss: 0.0768 - mse: 0.0768\n",
      "Epoch 2/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0463 - mse: 0.0463\n",
      "Epoch 3/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0313 - mse: 0.0313\n",
      "Epoch 4/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0253 - mse: 0.0253\n",
      "Epoch 5/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0291 - mse: 0.0291\n",
      "Epoch 6/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0287 - mse: 0.0287\n",
      "Epoch 7/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0266 - mse: 0.0266\n",
      "Epoch 8/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0263 - mse: 0.0263\n",
      "Epoch 9/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0256 - mse: 0.0256\n",
      "Epoch 10/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0290 - mse: 0.0290\n",
      "Epoch 11/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0246 - mse: 0.0246\n",
      "Epoch 12/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0264 - mse: 0.0264\n",
      "Epoch 13/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0239 - mse: 0.0239\n",
      "Epoch 14/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0222 - mse: 0.0222\n",
      "Epoch 15/90\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0230 - mse: 0.0230\n",
      "Epoch 16/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0198 - mse: 0.0198\n",
      "Epoch 17/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 18/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0199 - mse: 0.0199\n",
      "Epoch 19/90\n",
      "3/3 [==============================] - 6s 1s/step - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 20/90\n",
      "3/3 [==============================] - 6s 1s/step - loss: 0.0152 - mse: 0.0152\n",
      "Epoch 21/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 22/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0153 - mse: 0.0153\n",
      "Epoch 23/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0135 - mse: 0.0135\n",
      "Epoch 24/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0136 - mse: 0.0136\n",
      "Epoch 25/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0135 - mse: 0.0135\n",
      "Epoch 26/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0093 - mse: 0.0093\n",
      "Epoch 27/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0107 - mse: 0.0107\n",
      "Epoch 28/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0112 - mse: 0.0112\n",
      "Epoch 29/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0106 - mse: 0.0106\n",
      "Epoch 30/90\n",
      "3/3 [==============================] - 6s 1s/step - loss: 0.0090 - mse: 0.0090\n",
      "Epoch 31/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 32/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0100 - mse: 0.0100\n",
      "Epoch 33/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0087 - mse: 0.0087\n",
      "Epoch 34/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0098 - mse: 0.0098\n",
      "Epoch 35/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0096 - mse: 0.0096\n",
      "Epoch 36/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0161 - mse: 0.0161\n",
      "Epoch 37/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0155 - mse: 0.0155\n",
      "Epoch 38/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0105 - mse: 0.0105\n",
      "Epoch 39/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0103 - mse: 0.0103\n",
      "Epoch 40/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0096 - mse: 0.0096\n",
      "Epoch 41/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0116 - mse: 0.0116\n",
      "Epoch 42/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0090 - mse: 0.0090\n",
      "Epoch 43/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0079 - mse: 0.0079\n",
      "Epoch 44/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0089 - mse: 0.0089\n",
      "Epoch 45/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0070 - mse: 0.0070\n",
      "Epoch 46/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0068 - mse: 0.0068\n",
      "Epoch 47/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0055 - mse: 0.0055\n",
      "Epoch 48/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0053 - mse: 0.0053\n",
      "Epoch 49/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0068 - mse: 0.0068\n",
      "Epoch 50/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0101 - mse: 0.0101\n",
      "Epoch 51/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 52/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 53/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0057 - mse: 0.0057\n",
      "Epoch 54/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0084 - mse: 0.0084\n",
      "Epoch 55/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0074 - mse: 0.0074\n",
      "Epoch 56/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0088 - mse: 0.0088\n",
      "Epoch 57/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0078 - mse: 0.0078\n",
      "Epoch 58/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0065 - mse: 0.0065\n",
      "Epoch 59/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0076 - mse: 0.0076\n",
      "Epoch 60/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0063 - mse: 0.0063\n",
      "Epoch 61/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0087 - mse: 0.0087\n",
      "Epoch 62/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 63/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0071 - mse: 0.0071\n",
      "Epoch 64/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0068 - mse: 0.0068\n",
      "Epoch 65/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0073 - mse: 0.0073\n",
      "Epoch 66/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0060 - mse: 0.0060\n",
      "Epoch 67/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0049 - mse: 0.0049\n",
      "Epoch 68/90\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0087 - mse: 0.0087\n",
      "Epoch 69/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0096 - mse: 0.0096\n",
      "Epoch 70/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0090 - mse: 0.0090\n",
      "Epoch 71/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0101 - mse: 0.0101\n",
      "Epoch 72/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0120 - mse: 0.0120\n",
      "Epoch 73/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0100 - mse: 0.0100\n",
      "Epoch 74/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0093 - mse: 0.0093\n",
      "Epoch 75/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0087 - mse: 0.0087\n",
      "Epoch 76/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0075 - mse: 0.0075\n",
      "Epoch 77/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0063 - mse: 0.0063\n",
      "Epoch 78/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0068 - mse: 0.0068\n",
      "Epoch 79/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0048 - mse: 0.0048\n",
      "Epoch 80/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0046 - mse: 0.0046\n",
      "Epoch 81/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0041 - mse: 0.0041\n",
      "Epoch 82/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0038 - mse: 0.0038\n",
      "Epoch 83/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0031 - mse: 0.0031\n",
      "Epoch 84/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0030 - mse: 0.0030\n",
      "Epoch 85/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0029 - mse: 0.0029\n",
      "Epoch 86/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0038 - mse: 0.0038\n",
      "Epoch 87/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0052 - mse: 0.0052\n",
      "Epoch 88/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0037 - mse: 0.0037\n",
      "Epoch 89/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0039 - mse: 0.0039\n",
      "Epoch 90/90\n",
      "3/3 [==============================] - 5s 1s/step - loss: 0.0047 - mse: 0.0047\n",
      "[0.07338118553161621, 0.04506523907184601, 0.03237772360444069, 0.02745891734957695, 0.027026087045669556, 0.02971794269979, 0.028273070231080055, 0.026833537966012955, 0.026621295139193535, 0.028323551639914513, 0.025996003299951553, 0.02618565410375595, 0.023918071761727333, 0.023141508921980858, 0.021739685907959938, 0.020630262792110443, 0.02016099914908409, 0.022554881870746613, 0.020297657698392868, 0.016180967912077904, 0.016794707626104355, 0.015131944790482521, 0.013487648218870163, 0.013307438232004642, 0.013322572223842144, 0.009832845069468021, 0.01109627727419138, 0.011122760362923145, 0.00986022874712944, 0.008968312293291092, 0.007397357374429703, 0.010912240482866764, 0.008139199577271938, 0.009933503344655037, 0.009560364298522472, 0.016286710277199745, 0.014492660760879517, 0.010558565147221088, 0.011248543858528137, 0.00934987235814333, 0.011196639388799667, 0.009614288806915283, 0.008394790813326836, 0.008587706834077835, 0.007217273116111755, 0.006497891619801521, 0.006031988188624382, 0.0053819091990590096, 0.0073895747773349285, 0.009935446083545685, 0.008796713314950466, 0.007449320051819086, 0.005585753824561834, 0.008877396583557129, 0.007188230287283659, 0.008918631821870804, 0.00774226151406765, 0.006651722360402346, 0.00784575566649437, 0.006388249807059765, 0.008876534178853035, 0.007000845856964588, 0.007441241294145584, 0.007081721443682909, 0.007220998406410217, 0.0059381513856351376, 0.005185834132134914, 0.008657816797494888, 0.00968611054122448, 0.009075958281755447, 0.010461688041687012, 0.012762888334691525, 0.010164543054997921, 0.010114393197000027, 0.008502759970724583, 0.007661781273782253, 0.006310465279966593, 0.006507083773612976, 0.004829843528568745, 0.004468528553843498, 0.0040801516734063625, 0.003845250466838479, 0.003332966472953558, 0.0028746137395501137, 0.0029134887736290693, 0.00398726062849164, 0.005113585386425257, 0.0038988208398222923, 0.004389527253806591, 0.004991515539586544]\n"
     ]
    }
   ],
   "source": [
    "    #y_to = keras.utils.to_categorical(y_to,num_classes=4)\n",
    "history = model.fit(x=[input_to_model,age],y=ground_truth, epochs = 90 , batch_size = 64)\n",
    "print(history.history['loss'])\n",
    "#epoch_loss += history.history['loss'][0]\n",
    "#epoch_accu += history.history['accuracy'][0]\n",
    "\n",
    "#loss_hist.append(history.history['loss'])\n",
    "#accu_hist.append(history.history['accuracy'])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AqWZ3Mf1I28",
    "outputId": "5abf322a-f48c-4c9c-b20f-47d4e1672085"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\nplt.plot(accu_hist)\\nplt.title('model accuracy')\\nplt.ylabel('accuracy')\\nplt.xlabel('epoch')\\ns = '../working/accuracy_plot_' + str(epochs)\\nplt.savefig(s)\\nplt.show()\\nplt.close()\\n\\nplt.plot(loss_hist)\\nplt.title('model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\ns = '../working/loss_plot_' + str(epochs)\\nplt.savefig(s)\\nplt.show()\\nplt.close()\\n\\nmodel.save('../working/2d_4class_axis1.h5')\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('/home/vivek/Desktop/BTP_2020/Segmentation/Brain-Tumor-Segmentation-and-Survival-Prediction-using-Deep-Neural-Networks/working/surv_pred.h5')\n",
    "#epoch_loss = epoch_loss/180\n",
    "#epoch_accu = epoch_accu/180\n",
    "\n",
    "#epoch_wise_loss.append(epoch_loss)\n",
    "#epoch_wise_accu.append(epoch_accu)\n",
    " \n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model_loss vs epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epochs')\n",
    "s = '../working/epochwise_loss'\n",
    "plt.savefig(s)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#plt.plot(history.history['mse'])\n",
    "#plt.title('Model_Accuracy vs epochs')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.xlabel('epochs')\n",
    "#s = '../working/epochwise_accuracy'\n",
    "#plt.savefig(s)\n",
    "#plt.show()\n",
    "#plt.close()\n",
    "\n",
    "'''\n",
    "plt.plot(accu_hist)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "s = '../working/accuracy_plot_' + str(epochs)\n",
    "plt.savefig(s)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "s = '../working/loss_plot_' + str(epochs)\n",
    "plt.savefig(s)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "model.save('../working/2d_4class_axis1.h5')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/vivek/Desktop/BTP_2020/Segmentation/Brain-Tumor-Segmentation-and-Survival-Prediction-using-Deep-Neural-Networks/working/surv_pred.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "surv-pred-regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
